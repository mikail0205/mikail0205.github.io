---
title: CS231n Lec01 Computer vision(컴퓨터 비전)
description: Youtube 온라인 강의 CS231n에 대한 번역
date: 2018-05-20
categories:
 - Computer Vision
 - Machine Learning
tags: ComputerVision, ML
---
## Computer Vision
컴퓨터 비전은 visual data[시각 데이터]에 대한 연구이다. Visual data의 양은 계속해서 증가하고 있는데, 2017년에 모든 인터넷의 80%에 해당하는 자료가 비디오가 될 것이다는 CISCO의 연구도 있다.
![](https://www.bleepstatic.com/content/hl-images/2017/03/20/Cisco.jpg)

Visual data는 문제가 하나 있는데, 바로 이해하기가 너무 어렵다는 것. Visual data를 인터넷의 암흑 물질(Dark matter)이라고 비유하기도 하는데. 암흑 물질은 우주의 질량의 많은 부분을 차지하고 있지만 천체에 대한 중력의 존재로 암흑 물질이 존재하는 것을 알뿐 직접적으로 관찰할 수는 없다.
> 암흑물질 (Dark Matter) : 우주의 총 에너지의 대략 23%를 차지하며, 나머지는 가시광선으로 관측할 수 있는 물질과 암흑 에너지(Dark energy)로 이루어진다는 것이 현재의 이론. 물질만을 고려하면 암흑 물질은 우주 전체 물질의 84.5%를 차지하며, 암흑에너지와 암흑물질은 전체 구성 중 약 96%를 차지한다.

이렇듯 인터넷의 visual data 역시 마찬가지인데, 인터넷을 둘러싼 대부분의 bits로 구성되어 있지만 알고리즘이 실제로 들어가서 웹상의 모든 visual data를 정확히 무엇이 구성하는지 이해하고 보는 것은 매우 어렵기 때문이다.

## History of Vision
### Biological vision
Vision에 대한 역사는 많은 시간을 거슬러 가야 하는데 (정확히는 5억 4300만 년 전) 그 당시 지구는 대부분 물로 덮여 있었고 아주 적은 생명만이 바다를 떠다녔을 뿐이었다. 그러다 놀라운 일이 5억 4000만 년 전에 발생하게 되는데. 화석을 연구하던 동물연구 학자가 매우 짧은 기간 동안(1000만 년) 동물의 종류가 폭발적으로 증가했다는 것을 찾아낸다.

겨우 몇 종에서 수십만 종으로 넘어가는 과정은 분명 이상한 일일 것이다. 과연 무엇이 이 현상을 만들었을까? 많은 이론이 있기는 하지만 오랜 기간 동안 미스터리로 남았었다. 진화 생물학자들은 이것을 "진화의 빅뱅"이라고 부르는데, 호주 동물학자인 Andrew Parker가 화석 연구 중 바로 약 5억 4000만 년 전에 첫 번째 동물의 눈과 시력이 발달함에 따라 이 폭발적인 진화 단계가 시작되었다는 가장 설득력 있는 이론을 제시하게 된다.

일단 볼 수 있게 되면 삶은 훨씬 더 능동적이게 된다. 일부 포식자는 사냥을 하고 피식자는 포식자로부터 도망쳐야 하므로 시력의 진화 또는 발명은 진화론 적 군비 경쟁을 시작하게 하였으며 종족으로 살아남기 위해서는 동물이 빠르게 진화해야만 했다. 그리고 이것이 동물의 vision의 시작이다.

5억 4000만 년이 지난 후에 시각은 거의 모든 동물, 특히 영리한 동물의 가장 큰 감각 기관으로 발전했는데.
>인간은 시각 처리에 관련된 피질(cortex)의 거의 50%에 해당하는 뉴런을 가지고 있다.

우리가 생존하고, 일하고, 움직이고, 조작하고, 의사소통하고 그 외 많은 것들을 가능하게 하는 가장 큰 감각 시스템으로 시각은 동물 (특히 영리한 동물)에게는 정말로 중요하다.

### Human vision
기계적인 시각 혹은 카메라를 만드는 인간의 역사는 어떨까? 초기 카메라 중 하나는 1600년대 르네상스 시대의 카메라 'Obscura'에서 나온 것이다.
![](http://www.creativephotography.org/exhibits/richard-torchia/trchedu/diagrams/coenxx.gif "Obscura")
이 카메라는 핀홀(Pinhole) 카메라 이론에 기반을 둔 카메라이며 동물들이 빛을 모으기 위해 발달한 초기의 눈과 매우 유사하다. 생물 학자들은 시력의 메커니즘을 연구하기 시작했는데, computer vision뿐만 아니라 동물의 시각과 인간 시각의 가장 영향력 있는 작업 중 하나는 전기생리학(electrophysiology)을 사용하는 1950~60년대 Hubel과 Wiesel이 수행한 작업이다. 그들이 물었던 질문은
> 영장류와 포유류에서와 같은 시각적 메커니즘은 무엇인가?였다.

그래서 그들은 시각적 처리 관점에서 인간의 뇌와 어느 정도 유사한 고양이 뇌를 연구하기로 결심했으며, 그들이 한 일은 주요 시각 피질 영역이 있는 고양이 두뇌의 뒷부분에 전극을 찔러 넣고. 그 다음 고양이 뇌의 주요 시각 피질의 뒷부분의 뉴런을 흥분시키는 자극을 보는 것이었다.
![](https://user-images.githubusercontent.com/32008883/30751966-dea3b4c2-9ff5-11e7-870d-44140ff3b480.JPG)

그렇게 알게 된 사실은 고양이 두뇌의 주요 시각 피질 부분에 많은 종류의 세포가 있다는 것이었다. 그러나 가장 중요한 세포 중 하나는 특정 방향으로 움직일 때 지향성(oriented) edge에 반응하는 단순 세포였다. 물론 더 복잡한 세포도 있지만, 그들이 발견한 것은 시각적인 처리가 시각적 세계의 단순한 구조, 지향적인 윤곽기반에서부터 시작되고 정보가 시각적 처리 경로를 따라 이동함에 따라 뇌가 복잡한 시각 세계를 인식할 때까지 시각 정보의 복잡성을 구축하게 된다.

### Computer vision

따라서 컴퓨터 비전의 역사 역시 1960 년대 초반부터 시작된다. 'Block world'는 Larry Roberts가 출판 한 작품으로, 시각적 세계가 단순한 기하학적 모양으로 단순화되고 목표가 인식 할 수 있는 컴퓨터 비전의 첫 번째 PHD 학위 논문 중 하나로 널리 알려져 있다.
![](http://www.packet.cc/images/mach-per-fig2.jpg "Block world")

1966 년에 "Summer Vision Project"라는 유명한 MIT 여름 프로젝트가 있었는데. 이 프로젝트의 목표는 visual system의 중요한 부분을 구성하는 데 바로 여름 노동자들을 효율적으로 사용하려는 시도였다. 50여 년이 지난 지금도 컴퓨터 비전 분야는 한 여름 프로젝트에서 전 세계 수 천명의 연구자들로 피어나고 있으며, 여전히 vision의 가장 근본적인 문제 중 일부를 다루고 있다. 아직 vision을 해결하지 못했지만 인공 지능 분야에서 가장 중요하고 빠르게 성장하는 분야 중 하나가 되었다.

또 한 명의 경의를 표해야 할 사람은 'David Marr'이다. MIT 과학자로 1970년대 후반에 그의 vision에 대한 생각과 우리가 computer vision에 대해 어떻게 접근해야 하는지 그리고 컴퓨터로 시각적 세계(visual world)를 인식할 수 있게 하는 알고리즘을 개발하는 것에 대한 영향력 있는 책을 저술했다. 그 책의 내용은 이미지를 취하고 시각적 세계의 마지막인 3차원 표현에 도달하기 위해 여러 과정을 거쳐야 한다는 것이다.

![](http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/GOMES1/img1.gif)

첫 번째 과정은 '원시 스케치 (primal sketch)'라고 부른다. 대부분 가장자리, 바, 끝, 가상 선, 커브, 경계가 표현되는 곳이며 신경과학자(neuroscientist)들이 보았던 것에서 영감을 받았다. Hubel과 Wiesel은 시각적 처리의 초기 단계가 모서리(edge)와 같은 간단한 구조와 관련이 있다고 말했다. 다음 단계는 "2-and-half-D sketch"라고 부른다. visual scene의 surfaces, depth information, layers 또는 시각 화면의 불연속 부분을 함께 조각하기 시작한 다음 결국 모든 것을 모아서 surface 및 volumetric primitives 등을 통해 계층적으로(hierarchically organized) 구성된 3차원 모델을 갖게 되는데 이것은 vision이 무엇인지와 그러한 사고방식에 대한 매우 이상적인 사고 과정이었다. 실제로 수십 년 동안 computer vision을 지배해 왔으며 학생들이 시각 영역(field of visual)에 들어가서 시각적 정보를 분석하는 방법을 생각하는 데 매우 직관적인 방법이기도 하다.

1970년대에 또 하나의 매우 중요한 세미나 그룹이 생겨났다. 사람들이 "단순한 블록 세계를 넘어서 어떻게 현실 세계의 대상을 인식하고 표현할 수 있을까?"하는 질문을 하기 시작했다. 그 당시를 생각해보면, 사용 가능한 데이터가 거의 없다시피 했으며 컴퓨터는 느리고 PC도 주변에 있지 않았지만 컴퓨터 과학자들은 우리가 어떻게 물체를 인식하고 표현할 수 있는지 생각하기 시작했다. 그래서 스탠포드와 SRI(Standford Research Insititue) 두 그룹에서 과학자들이 유사한 아이디어를 제안했는데 하나는 일반화된 실린더(generalized cylinder) 다른 하나는 그림구조(pictorial structure)라고 불렀다.

![](https://user-images.githubusercontent.com/32008883/30751987-f03d50e4-9ff5-11e7-8650-70da272845b6.JPG)

기본 개념은 모든 객체가 단순한 기하학적 기본 요소로 구성된다는 것이다. 예를 들어 사람은 "일반화된 실린더" 모양으로 함께 결합될 수 있거나 혹은 사람은 그들의 탄성 거리(elastic distance)에 있는 핵심 부분(critical part)에 의해 결합될 수 있다는 것. 따라서 어느 표현이라도 객체의 복잡한 구조를 더 단순한 형태의 집합과 기하학적 구성으로 축소하는 방법이다.

이런 작업들은 몇년간 꽤나 영향력이 있었으며 1980년대에는 David Lowe가 단순한 세계 구조에서 시각적 세계를 재구성하거나 인식하는 방법을 생각하기도 했다.

그 당시 computer vision의 과제가 무엇인지 생각하려는 많은 노력들이 있었는데 1960 ~ 80년대에는 물체 인식에 대한 문제를 풀기가 너무 어려웠다. 위의 내용들은 분명 야심차고 대담한 시도이지만 장난 수준에 불과하다. 현실세계에서 작동할 수 있는 무언가를 제공한다는 측면에서는 많은 진전이 없었기 때문인데. 그래서 사람들은 vision을 해결하는 데 있어 무엇이 문제인지 생각하게 되는데 한 가지 중요한 내용은 다음과 같다.
> 객체 인식이 너무 어려울 경우, 객체 분할을 먼저 수행해야 한다. 즉, 이미지를 가져와서 픽셀을 의미 있는 영역으로 그룹화해야 하는 것.

우리가 그룹으로 묶인 픽셀을 사람이라고 부를 수 있는지 알 수 없지만 그 배경을 보고 사람에게 속한 모든 픽셀들을 추출할 수는 있는데. 이것을 영상분할(image segmentation)이라고 한다.
> image segmentation : Computer vision에서 분할은 디지털 영상을 여러개의 픽셀 집합으로 나누는 과정을 말하는데 그 의미는 영상의 표현을 좀 더 의미있고 해석하기 쉬운 것으로 단순화하거나 변환하는 것이다.

영상분할 문제에 대한 그래픽 이론 알고리즘을 사용한 아주 초기의 작품이 있는데. Computer vision의 얼굴 인식에 대한 문제이다. 1999년에서 2000년 사이에 machine learning이 추진력을 얻기 시작하는데. 이는 vector machine, boosting과 neural network(신경망)의 첫 번째 물결을 포함한 그래픽 모델이다. 많은 기여를 한 작업은 AdaBoost 알고리즘을 사용한 Paul Viola와 Micheal Jones의 실시간으로 얼굴을 탐지하는 작업이었다. 2001년에는 컴퓨터 칩이 느렸지만 거의 실시간으로 이미지에서 얼굴을 감지 할 수 있었고 2006년 5월 신문에 발표한 후 Fujifilm은 실시간 얼굴 탐지가 가능한 최초의 디지털 카메라를 출시하게된다.

기초 과학 연구에서 실제 세상에 응용까지 매우 빠른 인계였으며 어떻게 하면 물체를 더 잘 인식할 수 있을지 지속적으로 연구하고 있다. 1990년대 후반부터 2000년대 초반까지 가장 영향력 있었던 사고방식 중 하나는 feature 기반의 물체 인식이었다. SIFT feature라고 불리는 David Lowe의 작업이 있는데.
![](https://user-images.githubusercontent.com/32008883/30752062-1eba20aa-9ff6-11e7-92d5-38e5bb037353.JPG)
예를 들자면 정지신호가 있는데 문제는 이 정지신호를 다른 정지신호와 일치시키는 일은 매우 어려운데 바로 카메라의 각도, 시점, 조명 및 객체 자체의 고유한 변형으로 인해 그렇다. 하지만 관찰하던 중 객체에는 변화에 영향을 받지 않는 어떠한 특징이 있다는 사실에 영감을 얻게 되면서 객체 인식은 중요한 feature를 식별하고 유사 객체와 feature를 일치시키는 것으로 시작이 된다. 이는 전체 객체를 일치시키는 것보다 쉬운 작업이기 때문이다. 따라서 위 그림은 하나의 정지 표시로부터 SIFT 특징들이 식별되며 다른 정지 표지의 SIFT 특징들과 일치한다는 것을 보여주는 그림이다.

한 걸음 더 나아가 feature를 사용하여 전체론적(holistic) 장면을 인식하기 시작했다. 한 예로 'Spatial Pyramid Matching'이라 불리는 알고리즘이 있는데.
![](https://www.researchgate.net/profile/Michael_Mayo4/publication/254051049/figure/fig1/AS:298045649506304@1448071028274/Spatial-pyramid-matching.png)
이 아이디어는 풍경이나, 부엌, 고속도로 등 어떤 장면 유형이든 상관없이 feature가 있고 그 장면이 무엇인지에 대한 단서를 제공한다는 것이다. 이 작업은 이미지의 다른 부분과 다른 해상도에서 feature를 가져와 feature discriptor에 넣고 그 위에 vector machine 알고리즘을 사용하는 것이다. 매우 유사한 연구가 인간 인지(human recognition)에 힘을 얻고 있다. 이러한 feature를 잘 정리하면 보다 현실적인 이미지로 인체를 구성하고 인식할 수 있는 방법을 모색하는 많은 작업을 수행할 수 있다. ("Histogram of gradients", "deformable part models"와 같은 영상 feature 방법이 있다.)
> feature descriptor : 유용한 정보를 추출하고 불필요한 정보를 버림으로써 이미지를 단순화하는 이미지 또는 이미지 패치 표현.

1960년대를 지나 21세기 초까지, 한가지 변화는 사진의 품질이다. 인터넷의 성장과 함께 디지털카메라는 computer vision을 연구하기 위해 더 좋은 데이터를 가지게 되었고 2000년대 초의 결과 중 하나는 computer vision 분야가 해결해야 할 중요한 feature 문제를 정의했다는 것이다. 물론 이것은 해결해야 할 유일한 문제가 아니지만 인식의 측면에서, 객체 인식(object recognition)을 해결하는 매우 중요한 문제이다. 지금까지 객체 인식에 대해서 계속 이야기하고 있는데. 스탠퍼드에서 2000년대 초반에 객체 인식의 진행 상황을 측정할 수 있는 벤치마크 데이터셋을 만들기 시작한다. 바로 가장 영향력 있는 벤치마크 데이트셋 중 하나인 **"PASCAL Visual Object Challenge"**이다. 20개의 객체 클레스들로 구성된 데이터셋이며 기차, 비행기, 사람, 고양이, 소 등이 있고 데이터셋의 범주당 수천에서 수만 가지의 이미지로 구성되며 field마다 testing set에 대해 테스트를 하고 진행 상황을 확인하는 알고리즘을 개발하게 된다.
![](https://github.com/mikail0205/mikail0205.github.io/blob/master/assets/images/2018/pvoc2012.png?raw=true)
여기 2007 ~ 2012년까지 보여주는 차트가 있는데. 벤치마크 데이터셋에서 이미지의 object 감지 성능이 꾸준히 증가하는 걸 볼 수 있듯. 많은 진전이 있을 즘에 Princeton에서 Stanford에 이르기까지 스스로에게 더 어려운 질문을 하기 시작한다.
> 즉 "과연 우리가 세상의 모든 것을 인식할 준비가 되어있는가?"이다.

Machine learning 알고리즘의 대부분은 graphic model, support vector machine 또는 AdaBoost인지 여부는 중요하지 않다. 교육 과정에서 지나치게 적합(overfit) 할 가능성이 높으며 일부 문제는 시작 데이터가 매우 복잡하다는 것이다. 복잡하게 되면 모델은 높은 차원의 입력을 가지는 경향이 있고, 많은 매개 변수를 필요로 한다. 충분한 교육 데이터(training data)가 없을 때. 과적합(overfiting)은 매우 빠르게 일어나며 일반화(generalize) 하기가 어렵다. 두 가지 이유에 의해 동기부여가 되는데 하나는 세계의 모든 객체 인식하고 싶다는 것이고 다른 하나는 [machine learning으로 돌아와서] 과적합의 병목 현상을 극복하는 것이다. 그렇게 ImageNet이라는 프로젝트를 시작하게 된다.

![](https://www.fanyeong.com/wp-content/uploads/2018/01/v2-718f95df083b2d715ee29b018d9eb5c2_r.jpg)

> ImageNet 프로젝트는 시각적 객체 인식 소프트웨어 연구에 사용하도록 설계된 대형 시각적 데이트베이스이다. 1400만 개가 넘는 이미지 URL이 ImageNet에 의해 손으로 주석 처리되어 어떤 물체가 그려지는지 알려준다. 적어도 백만개의 이미지에서 bounding box가 제공되며 ImageNet에는 2만 개 이상의 모호한 범주가 있다.

우리는 세상에 존재하는 객체에 대해 찾을 수 있는 가능한 가장 큰 데이터셋을 만들고자 했으며 데이터셋을 사용하여 훈련하고 벤치마킹을 했다. 따라서 약 3년의 기간 동안 진행된 정말 어려운 프로젝트였다. 기본적으로 'WordNet'이라 부르는 수만 개의 object classes로 구성된 사전을 사용해서 정리된 수십억 개의 이미지를 인터넷에서 다운로드하는 걸 시작으로 한다.
> 워드넷(WordNet): 영어의 의미 어휘목록이다. 영어 단어를 'synset'이라는 유의어 집단으로 분류하여 간략하고 일반적인 정의를 제공하고, 이러한 어휘목록 사이의 다양한 의미 관계를 기록한다. 워드넷은 심리학 교수인 조지 A. 밀러가 지도하는 프린스턴 대학의 인지 과학 연구소에 의해 만들어졌고 유지되고 있다. 개발은 1985년에 시작되었다.

그런 다음 clever crowd engineering trick을 사용해야 했는데. Amazon Mechnical Turk platform을 사용해서 각각의 이미지를 sort, clean, label 했다. 최종 결과는 22,000개의 카테고리로 정리된 150만에서 400만 이상에 해당하는 물체와 장면에 대한 ImageNet으로 이것은 매우 거대하며(gigantic), 아마 그 당시에 인공지능 분야에서 만들어진 가장 큰 데이터셋일 것이다. 그리고 객체 인식에 대한 알고리즘을 다른 단계(phase)로 나아가도록 했다.

특히나 중요한 것은 "어떻게 과정을 벤치마크 할 것인가"인데. 2009년을 시작으로 ImageNet 팀은 국제적인 대회를 열었고 (ImageNet Large-Scale Visual Recognition Challenge)이 대회를 위해서 더욱 강화된 test set(140만 개의 객체와 1,000개의 object classes로 구성된)을 준비했다. 그리고 이것은 computer vision 알고리즘의 결과를 위해 이미지 분류 인식을 테스트 하기 위함이다.
![](https://github.com/mikail0205/mikail0205.github.io/blob/master/assets/images/2018/LSVRC.png?raw=true)

그리고 예제 사진처럼 만약 알고리즘의 결과가 5개의 label을 출력할 수 있고 상위 다섯 개의 label이 올바른 객체를 포함하고 있다면 '성공'이라 부른다.
![](https://www.researchgate.net/profile/Gustav_Von_Zitzewitz/publication/324476862/figure/fig7/AS:614545865310213@1523530560584/Winner-results-of-the-ImageNet-large-scale-visual-recognition-challenge-LSVRC-of-the.png)

ImageNet challenge 이미지 분류 결과로 (2011 ~ 2016년) 좋은 소식은 에러율이 지속적으로 줄어들고 2012년에 큰 폭으로 줄어드는데 비록 이미지 인식에(이번 강의에서 배우게 될) 대한 모든 문제를 풀지는 못 했지만 엄청난 진전을 이루었으며 실세계 응용 프로그램이 ImageNet 도전과제에서 사람과 **(여기서 사람이란 한 명의 스탠포드 대학 박사학위 학생인데 마치 대회에 참여하는 컴퓨터인 것처럼 몇 주간 일을 했다.)** 동등한 수준으로 오류율을 좁히는 데는 몇 년 밖에 걸리지 않았다. 그리고 특별히 주목해야 할 순간은 2012년인데. 처음 2년 동안은 오류율가 25%에 머물렀던 것이 2012년에는 10%나 떨어져 16%가 되었다. 물론 지금은 더 좋지만 그만한 감소율은 significant하고 그 해 우승을 차지한 알고리즘은 CNN( convolutional neural network) 모델인데 다른 알고리즘을 다 물리치고 ImageNet Challenge의 우승을 차지했다. 이 강의는 deep learning 이라고도 불리는 CNN모델에 대해 심층적으로 다루며 이 모델들이 무엇인지, 무엇이 좋은지, 그리고 이 모델의 최근 진행 상황은 무엇인지를 공부할 것이다. 역사가 만들어진 곳은 2012년 CNN 모델인데. 자연어 처리 및 음성 인식과 같은 다른 여러 자매 분야와 함께 computer vision에서 좋은 발전을 이루는데 막대한 역량과 능력을 보여주었기 때문이다.



## Release note
2018-05-20 : First upload  
2018-05-22 : Update  
2018-05-26 : Update


## References
[CS231n]<https://www.youtube.com/watch?v=vT1JzLTH4G4&list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk>